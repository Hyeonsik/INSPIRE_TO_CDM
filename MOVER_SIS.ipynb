{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4cf2d5-85b4-40cb-90f2-b68af701cdd2",
   "metadata": {},
   "source": [
    "# Loading MOVER SIS_EMR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf7a8ade-f14a-4bc8-b22d-9a391726956c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T06:03:10.704550Z",
     "iopub.status.busy": "2023-11-17T06:03:10.704209Z",
     "iopub.status.idle": "2023-11-17T06:03:23.783800Z",
     "shell.execute_reply": "2023-11-17T06:03:23.782819Z",
     "shell.execute_reply.started": "2023-11-17T06:03:10.704517Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1362697/2653913319.py:16: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_obs = pd.read_csv(f'{input_path}/patient_observations.csv', on_bad_lines='skip')    # Load observations table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tables: information 19114, medication 373852,      labs 14733, input_output 100993, vitals 3847548, observations 3663066,      ventilator 1048575, aline 2989, events 40801\n",
      "total subjects in MOVER dataset: 19158\n",
      "total subjects in patient_information.csv: 19114\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# MOVER (about 39,,685 patients, 64,354 cases)\n",
    "# Define the path to the INSPIRE v2 dataset\n",
    "input_path = 'mover/SIS_EMR'\n",
    "\n",
    "# Load the source tables within INSPIRE v2 into dataframes\n",
    "df_info = pd.read_csv(f'{input_path}/patient_information.csv')         # Load information data\n",
    "df_medi = pd.read_csv(f'{input_path}/patient_medication.csv')       # Load medications data\n",
    "df_labs = pd.read_csv(f'{input_path}/patient_labs.csv')       # Load labs data\n",
    "df_io = pd.read_csv(f'{input_path}/patient_input_output.csv')       # Load input_output data\n",
    "df_vitals = pd.read_csv(f'{input_path}/patient_vitals.csv', on_bad_lines='skip')          # Load  vitals data\n",
    "df_obs = pd.read_csv(f'{input_path}/patient_observations.csv', on_bad_lines='skip')    # Load observations table\n",
    "df_vent = pd.read_csv(f'{input_path}/patient_ventilator.csv', on_bad_lines='skip')\n",
    "df_aline = pd.read_csv(f'{input_path}/patient_a_line.csv', on_bad_lines='skip')\n",
    "df_events = pd.read_csv(f'{input_path}/patient_procedure_events.csv', on_bad_lines='skip')          # Load vitals data\n",
    "\n",
    "\n",
    "# Display the number of records in each dataset\n",
    "print(f'Size of the tables: information {len(df_info)}, medication {len(df_medi)},\\\n",
    "      labs {len(df_labs)}, input_output {len(df_io)}, vitals {len(df_vitals)}, observations {len(df_obs)},\\\n",
    "      ventilator {len(df_vent)}, aline {len(df_aline)}, events {len(df_events)}')\n",
    "\n",
    "# Combine all the subject_ids from the loaded datasets\n",
    "subject_ids = df_io['PID'].tolist() + df_labs['PID'].tolist() + df_medi['PID'].tolist() + df_info['PID'].tolist() + df_obs['PID'].tolist() + df_events['PID'].tolist() + df_vent['PID'].tolist() + df_aline['PID'].tolist()\n",
    "\n",
    "# Display the total unique subjects present in the combined dataset\n",
    "print(f'total subjects in MOVER dataset: {len(np.unique(subject_ids))}')\n",
    "print(f\"total subjects in patient_information.csv: {len(np.unique(df_info['PID']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5285f68-6b57-4d59-8e37-6c6b88b7f111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T06:32:42.845436Z",
     "iopub.status.busy": "2023-11-17T06:32:42.845079Z",
     "iopub.status.idle": "2023-11-17T06:33:16.292469Z",
     "shell.execute_reply": "2023-11-17T06:33:16.291693Z",
     "shell.execute_reply.started": "2023-11-17T06:32:42.845401Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1362697/1270202596.py:3: DtypeWarning: Columns (5,6,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_concept = pd.read_csv(f'vocab/CONCEPT.csv', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# Load Athena concept, concept_relationship table\n",
    "df_concept_rel = pd.read_csv(f'vocab/CONCEPT_RELATIONSHIP.csv', sep='\\t', on_bad_lines='error')\n",
    "df_concept = pd.read_csv(f'vocab/CONCEPT.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced5b056-bdef-4691-a3f8-420470699657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T06:13:31.157140Z",
     "iopub.status.busy": "2023-11-17T06:13:31.156984Z",
     "iopub.status.idle": "2023-11-17T06:13:31.161540Z",
     "shell.execute_reply": "2023-11-17T06:13:31.160779Z",
     "shell.execute_reply.started": "2023-11-17T06:13:31.157125Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tables: information 19114, medication 373616,      labs 14733, input_output 100967, vitals 3595596, observations 2684312,      ventilator 1048575, aline 2951, events 40697\n"
     ]
    }
   ],
   "source": [
    "df_info = df_info.drop_duplicates().reset_index(drop=True)\n",
    "df_io = df_io.drop_duplicates().reset_index(drop=True)\n",
    "df_medi = df_medi.drop_duplicates().reset_index(drop=True)\n",
    "df_vitals = df_vitals.drop_duplicates().reset_index(drop=True)\n",
    "df_events = df_events.drop_duplicates().reset_index(drop=True)\n",
    "df_labs = df_labs.drop_duplicates().reset_index(drop=True)\n",
    "df_obs = df_obs.drop_duplicates().reset_index(drop=True)\n",
    "df_vent = df_vent.drop_duplicates().reset_index(drop=True)\n",
    "df_aline = df_aline.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Display the number of records in each dataset\n",
    "print(f'Size of the tables: information {len(df_info)}, medication {len(df_medi)},\\\n",
    "      labs {len(df_labs)}, input_output {len(df_io)}, vitals {len(df_vitals)}, observations {len(df_obs)},\\\n",
    "      ventilator {len(df_vent)}, aline {len(df_aline)}, events {len(df_events)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03903592-8549-4f00-a297-03c541478a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T06:18:53.357930Z",
     "iopub.status.busy": "2023-11-17T06:18:53.357434Z",
     "iopub.status.idle": "2023-11-17T06:18:53.450356Z",
     "shell.execute_reply": "2023-11-17T06:18:53.449707Z",
     "shell.execute_reply.started": "2023-11-17T06:18:53.357894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### patient_information table\n",
    "# Convert the datetime in string into datetime64[ns] type\n",
    "df_info['OR_start'] = pd.to_datetime(df_info['OR_start'], format='%m/%d/%y %H:%M', errors='raise')\n",
    "df_info['OR_end'] = pd.to_datetime(df_info['OR_end'], format='%m/%d/%y %H:%M', errors='raise')\n",
    "df_info['Surgery_start'] = pd.to_datetime(df_info['Surgery_start'], format='%m/%d/%y %H:%M', errors='raise')\n",
    "df_info['Surgery_end'] = pd.to_datetime(df_info['Surgery_end'], format='%m/%d/%y %H:%M', errors='raise')\n",
    "\n",
    "# Change string into numeric value for height and weight\n",
    "df_info['Ht'] = pd.to_numeric(df_info['Ht'], errors='coerce')\n",
    "df_info['Wt'] = pd.to_numeric(df_info['Wt'], errors='coerce')\n",
    "\n",
    "df_info.to_parquet(f'{input_path}/parquet/patient_information.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dea1af-7aa0-4f3c-b2b6-d506ec7945b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T03:26:52.459336Z",
     "iopub.status.busy": "2023-11-19T03:26:52.458874Z",
     "iopub.status.idle": "2023-11-19T03:27:00.336677Z",
     "shell.execute_reply": "2023-11-19T03:27:00.335961Z",
     "shell.execute_reply.started": "2023-11-19T03:26:52.459302Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### patient medication table\n",
    "# Original columns of medication table\n",
    "columns = list(df_medi.columns)\n",
    "\n",
    "# Regular expression to extract the string before a number or '/'\n",
    "df_medi['ingredient'] = df_medi['Drug_name'].str.extract(r'([^0-9/]+)')\n",
    "# Lower cases and strip whitespace\n",
    "df_medi['ingredient'] = df_medi['ingredient'].str.lower().str.strip()\n",
    "\n",
    "# Split ingredients that are coupled by '-'\n",
    "df_medi = df_medi.assign(ingredient=df_medi['ingredient'].str.split('-')).explode('ingredient').reset_index(drop=True)\n",
    "df_medi['ingredient'] = df_medi['ingredient'].str.strip()\n",
    "\n",
    "# Matches the ingredient names with 1, 2, 3 words\n",
    "df_medi['ingredients'] = df_medi['ingredient'].str.split()\n",
    "df_medi['ingredient3'] = df_medi['ingredients'].apply(lambda x: ' '.join(x[:3])).str.strip()\n",
    "df_medi['ingredient2'] = df_medi['ingredients'].apply(lambda x: ' '.join(x[:2])).str.strip()\n",
    "df_medi['ingredient1'] = df_medi['ingredients'].apply(lambda x: ' '.join(x[:1])).str.strip()\n",
    "\n",
    "# Map the source concept of drugs into standard concept (RxNorm)\n",
    "rxnorm_concepts = df_concept[((df_concept['vocabulary_id'] == 'RxNorm') | (df_concept['vocabulary_id'] == 'RxNorm Extension')) & (df_concept['standard_concept'] == 'S')][['concept_name', 'concept_id']]\n",
    "rxnorm_concepts['concept_name'] = rxnorm_concepts['concept_name'].str.lower()\n",
    "\n",
    "df_medi['concept_id3'] = df_medi.merge(rxnorm_concepts, left_on='ingredient3', right_on='concept_name', how='left')['concept_id']\n",
    "df_medi['concept_id2'] = df_medi.merge(rxnorm_concepts, left_on='ingredient2', right_on='concept_name', how='left')['concept_id']\n",
    "df_medi['concept_id1'] = df_medi.merge(rxnorm_concepts, left_on='ingredient1', right_on='concept_name', how='left')['concept_id']\n",
    "\n",
    "df_medi['concept_id'] = df_medi[['concept_id3', 'concept_id2', 'concept_id1']].bfill(axis=1).iloc[:, 0]\n",
    "df_medi['concept_id'] = df_medi['concept_id'].astype('Int64')\n",
    "\n",
    "# Calculate and print the number of MEDICATIONS table records that couldn't be mapped to a standard concept\n",
    "nan_sum = df_medi['concept_id'].isna().sum()\n",
    "print(f'mismatched concepts in MEDICATIONS table: {nan_sum} / {len(df_medi)} ({nan_sum/len(df_medi)*100:.1f}%)')\n",
    "\n",
    "#df_medi.dropna(columns='concept_id', inplace=True)\n",
    "df_medi = df_medi[columns+['ingredient', 'concept_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc69c5ed-26c7-4ee4-b6bf-9098ea03f0a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T06:45:02.138422Z",
     "iopub.status.busy": "2023-11-17T06:45:02.137999Z",
     "iopub.status.idle": "2023-11-17T06:45:07.248963Z",
     "shell.execute_reply": "2023-11-17T06:45:07.248037Z",
     "shell.execute_reply.started": "2023-11-17T06:45:02.138387Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mismatched concepts in MEDICATIONS table: 133241 / 460664 (28.9%)\n"
     ]
    }
   ],
   "source": [
    "# previous method : just comparing exact concept name with ingredient\n",
    "df_medi.drop(columns='concept_id', inplace=True)\n",
    "\n",
    "### patient medication table\n",
    "# Regular expression to extract the string before a number or '/'\n",
    "df_medi['ingredient'] = df_medi['Drug_name'].str.extract(r'([^0-9/]+)')\n",
    "# Lower cases and strip whitespace\n",
    "df_medi['ingredient'] = df_medi['ingredient'].str.lower().str.strip()\n",
    "\n",
    "# Split ingredients that are coupled by '-'\n",
    "df_medi = df_medi.assign(ingredient=df_medi['ingredient'].str.split('-')).explode('ingredient').reset_index(drop=True)\n",
    "df_medi['ingredient'] = df_medi['ingredient'].str.strip()\n",
    "\n",
    "# Map the source concept of drugs into standard concept (RxNorm)\n",
    "drug_concept = df_concept[((df_concept['vocabulary_id'] == 'RxNorm') | (df_concept['vocabulary_id'] == 'RxNorm Extension')) & (df_concept['standard_concept'] == 'S')][['concept_name', 'concept_id']]\n",
    "drug_concept['concept_name'] = drug_concept['concept_name'].str.lower()\n",
    "\n",
    "df_medi['concept_id'] = df_medi.merge(drug_concept, left_on='ingredient', right_on='concept_name', how='left')['concept_id']\n",
    "\n",
    "# Calculate and print the number of MEDICATIONS table records that couldn't be mapped to a standard concept\n",
    "nan_sum = df_medi['concept_id'].isna().sum()\n",
    "\n",
    "#df_medi.dropna(columns='\n",
    "\n",
    "print(f'mismatched concepts in MEDICATIONS table: {nan_sum} / {len(df_medi)} ({nan_sum/len(df_medi)*100:.1f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107bb5a-f9c7-4610-9003-59e4d9b92249",
   "metadata": {},
   "source": [
    "* medi_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1283d19-7fe6-4d50-b8d9-9efb7ebd4a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T06:49:26.783069Z",
     "iopub.status.busy": "2023-11-17T06:49:26.782705Z",
     "iopub.status.idle": "2023-11-17T06:49:32.133271Z",
     "shell.execute_reply": "2023-11-17T06:49:32.132392Z",
     "shell.execute_reply.started": "2023-11-17T06:49:26.783034Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Medi_counts\n",
    "drugs_per_subject = df_medi.drop_duplicates(subset=['PID', 'ingredient'])\n",
    "drug_counts = drugs_per_subject['ingredient'].value_counts().to_frame()\n",
    "drug_counts.reset_index(inplace=True)\n",
    "# 문제는 ingredient 하나에 여러개 medication이 대응될 수 있음 (medication을 split해서 ingredient를 만들어서, 원래 ingredient 하나 자체가 medication인 경우와 중복)\n",
    "drug_counts['medication'] = drug_counts.merge(drugs_per_subject.drop_duplicates(['ingredient']), on='ingredient', how='left')['Drug_name']\n",
    "\n",
    "# Map the source concept of drugs into standard concept (RxNorm)\n",
    "rxnorm_concepts = df_concept[((df_concept['vocabulary_id'] == 'RxNorm') | (df_concept['vocabulary_id'] == 'RxNorm Extension')) & (df_concept['standard_concept'] == 'S')][['concept_name', 'concept_id']]\n",
    "rxnorm_concepts['concept_name'] = rxnorm_concepts['concept_name'].str.lower()\n",
    "\n",
    "drug_mapped = drug_counts.merge(rxnorm_concepts, left_on='ingredient', right_on='concept_name', how='left')\n",
    "\n",
    "drug_mismatch = drug_mapped[drug_mapped['concept_id'].isna()]\n",
    "\n",
    "drug_counts['perc'] = drug_counts['count'] / len(df_medi['PID'].unique()) * 100\n",
    "#drug_counts['mismatch'] = np.where(drug_counts['ingredient'].isin(drug_mismatch['ingredient']), 1, np.nan)\n",
    "#drug_counts.to_csv('results/mover_medi_counts.csv', index=False)\n",
    "\n",
    "# Remove rows that have empty ingredient\n",
    "drug_counts = drug_counts[drug_counts['ingredient']!='']\n",
    "\n",
    "# Add an auxiliary column to maintain the order\n",
    "drug_counts['order'] = range(len(drug_counts))\n",
    "\n",
    "# Step 1: Extract the first word of 'ingredient' if it has more than one word\n",
    "drug_counts['first_word'] = drug_counts['ingredient'].str.split().str[0]\n",
    "\n",
    "# Step 2: Merge with rxnorm_concepts on 'ingredient' for exact matches\n",
    "exact_matches = pd.merge(drug_counts, rxnorm_concepts, left_on='ingredient', right_on='concept_name', how='left')\n",
    "\n",
    "# Step 3: Identify which entries didn't get a match\n",
    "non_matches = drug_counts[~drug_counts['ingredient'].isin(exact_matches['concept_name'])]\n",
    "exact_matches.dropna(subset='concept_id', inplace=True)\n",
    "\n",
    "# Step 4: Merge the non-matches with the 'concept_name' based on 'first_word'\n",
    "partial_matches = pd.merge(non_matches, rxnorm_concepts, left_on='first_word', right_on='concept_name', how='left')\n",
    "\n",
    "# Step 5: Combine the exact matches with the partial matches\n",
    "combined_matches = pd.concat([exact_matches, partial_matches]).sort_values(by='order')\n",
    "\n",
    "# Step 6: Drop the temporary 'first_word' columns and any duplicates that might have arisen\n",
    "final_df = combined_matches.drop_duplicates(['first_word', 'order']).drop(columns=['first_word', 'order'])\n",
    "\n",
    "# Check mismatch\n",
    "final_df['mismatch'] = (final_df['concept_id'].isna()).astype(int)\n",
    "\n",
    "final_df.to_csv('results/mover_sis_medi_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02e04211-be1c-4678-ab41-c28c19f4deea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T03:29:46.987626Z",
     "iopub.status.busy": "2023-11-19T03:29:46.987276Z",
     "iopub.status.idle": "2023-11-19T03:29:47.065817Z",
     "shell.execute_reply": "2023-11-19T03:29:47.065060Z",
     "shell.execute_reply.started": "2023-11-19T03:29:46.987593Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 이어서 manually mapped medications 결과를 불러옴\n",
    "medi_mapped = pd.read_csv('results/mover_sis_medi_counts+manual.csv')\n",
    "\n",
    "df_medi['concept_id'] = df_medi.merge(medi_mapped, on='ingredient', suffixes=('_', None), how='left')['concept_id']\n",
    "\n",
    "# Convert string into datetime\n",
    "df_medi['Start_time'] = pd.to_datetime(df_medi['Start_time'], format='%m/%d/%y %H:%M', errors='raise')\n",
    "df_medi['End_time'] = pd.to_datetime(df_medi['End_time'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "# Save the result in parquet\n",
    "df_medi.dropna(subset=['Drug_name', 'Start_time'], inplace=True, ignore_index=True)\n",
    "df_medi.to_parquet(f'{input_path}/parquet/patient_medications.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "21895e54-0516-4bc7-9193-72a89a8e4cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T05:38:52.522939Z",
     "iopub.status.busy": "2023-11-19T05:38:52.522584Z",
     "iopub.status.idle": "2023-11-19T05:38:52.550005Z",
     "shell.execute_reply": "2023-11-19T05:38:52.549266Z",
     "shell.execute_reply.started": "2023-11-19T05:38:52.522906Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load manually mapped concepts\n",
    "df_params = pd.read_csv('mover/parameters_mover-sis_mapped.csv')\n",
    "\n",
    "\n",
    "### patient_labs table ###\n",
    "# Melt columns of labs table into rows\n",
    "labs_columns = list(df_labs.columns)\n",
    "df_labs = df_labs.melt(id_vars=labs_columns[:2], var_name='Label')\n",
    "\n",
    "# Merge manually mapped concepts into labs table\n",
    "labs_mapped = df_params[df_params['Table']=='labs']\n",
    "df_labs = df_labs.merge(labs_mapped[['Label', 'Unit', 'concept_id', 'unit_concept_id']], on='Label', how='left')\n",
    "\n",
    "# Convert string into datetime\n",
    "df_labs['Obs_time'] = pd.to_datetime(df_labs['Obs_time'].astype(str), format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Drop rows with nan value and save into parquet\n",
    "df_labs.dropna(subset=['Obs_time', 'Label', 'value', 'concept_id'], inplace=True, ignore_index=True)\n",
    "df_labs.to_parquet(f'{input_path}/parquet/patient_labs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "747d5ead-5441-4eb0-ae11-e8b3b4eee052",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T05:41:23.060487Z",
     "iopub.status.busy": "2023-11-19T05:41:23.060095Z",
     "iopub.status.idle": "2023-11-19T05:41:38.382908Z",
     "shell.execute_reply": "2023-11-19T05:41:38.382013Z",
     "shell.execute_reply.started": "2023-11-19T05:41:23.060453Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### patient vitals table ###\n",
    "# Melt columns of labs table into rows\n",
    "vitals_columns = list(df_vitals.columns)\n",
    "df_vitals = df_vitals.melt(id_vars=vitals_columns[:2], var_name='Label')\n",
    "\n",
    "# Merge manually mapped concepts into vitals table\n",
    "vitals_mapped = df_params[df_params['Table']=='vitals']\n",
    "df_vitals = df_vitals.merge(vitals_mapped[['Label', 'Unit', 'concept_id', 'unit_concept_id']], on='Label', how='left')\n",
    "\n",
    "# Convert string into datetime\n",
    "df_vitals['Obs_time'] = pd.to_datetime(df_vitals['Obs_time'].astype(str), format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Drop rows with nan value and save into parquet\n",
    "df_vitals.dropna(subset=['Obs_time', 'Label', 'value', 'concept_id'], inplace=True, ignore_index=True)\n",
    "df_vitals.to_parquet(f'{input_path}/parquet/patient_vitals.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0f5d7e49-42d6-4e34-9a53-2b95a92c3f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T06:09:24.822481Z",
     "iopub.status.busy": "2023-11-19T06:09:24.822076Z",
     "iopub.status.idle": "2023-11-19T06:09:27.068112Z",
     "shell.execute_reply": "2023-11-19T06:09:27.067190Z",
     "shell.execute_reply.started": "2023-11-19T06:09:24.822447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### patient observations table ###\n",
    "# Melt columns of labs table into rows\n",
    "obs_columns = list(df_obs.columns)\n",
    "df_obs = df_obs.melt(id_vars=obs_columns[:2], value_vars=obs_columns[2:], var_name='Label')\n",
    "\n",
    "# Merge manually mapped concepts into observation table\n",
    "obs_mapped = df_params[df_params['Table']=='observations']\n",
    "df_obs = df_obs.merge(obs_mapped[['Label', 'Unit', 'concept_id', 'unit_concept_id']], on='Label', how='left')\n",
    "\n",
    "# Convert string into datetime\n",
    "df_obs['Obs_time'] = pd.to_datetime(df_obs['Obs_time'].astype(str), format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "df_obs['value'] = pd.to_numeric(df_obs['value'].astype(str), errors='coerce')\n",
    "\n",
    "# Drop rows with nan value and save into parquet\n",
    "df_obs.dropna(subset=['Obs_time', 'Label', 'value', 'concept_id'], inplace=True, ignore_index=True)\n",
    "df_obs.to_parquet(f'{input_path}/parquet/patient_observations.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "abecc3bc-20ea-41e3-84c3-881bdfd3f832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T06:07:47.815052Z",
     "iopub.status.busy": "2023-11-19T06:07:47.814718Z",
     "iopub.status.idle": "2023-11-19T06:07:47.825498Z",
     "shell.execute_reply": "2023-11-19T06:07:47.824447Z",
     "shell.execute_reply.started": "2023-11-19T06:07:47.815020Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6162792 entries, 0 to 6162791\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   PID              object        \n",
      " 1   Obs_time         datetime64[ns]\n",
      " 2   Label            object        \n",
      " 3   value            float64       \n",
      " 4   Unit             object        \n",
      " 5   concept_id       object        \n",
      " 6   unit_concept_id  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(4)\n",
      "memory usage: 329.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_obs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8524772a-98db-4531-b2fa-d78c97ec8b38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T07:55:41.556979Z",
     "iopub.status.busy": "2023-11-19T07:55:41.556557Z",
     "iopub.status.idle": "2023-11-19T07:56:28.712813Z",
     "shell.execute_reply": "2023-11-19T07:56:28.711890Z",
     "shell.execute_reply.started": "2023-11-19T07:55:41.556944Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### patient ventilator table ###\n",
    "vent_columns = list(df_vent.columns)\n",
    "df_vent = df_vent.rename(columns={'ETC02':'ETCO2', 'FIC02': 'FICO2', 'FIN20': 'FIN2O', 'ETN20': 'ETN2O', 'FI02': 'FIO2', 'ET02':'ETO2'})\n",
    "df_vent = df_vent.melt(id_vars=vent_columns[:3], var_name='Label')\n",
    "\n",
    "\n",
    "# Convert Agent, Agent_Fi, Agent_Et into {Agent}_Fi, {Agent}_Et\n",
    "mask = df_vent['Label'].str.contains('Agent')\n",
    "df_vent.loc[mask, 'Label'] = df_vent.loc[mask, 'Agent'] + '_' + df_vent.loc[mask, 'Label'].str.split('_').str[1]\n",
    "# Drop unnecessary column\n",
    "df_vent = df_vent.drop('Agent', axis=1)\n",
    "\n",
    "# Merge manually mapped concepts into observation table\n",
    "vent_mapped = df_params[df_params['Table']=='ventilator']\n",
    "df_vent = df_vent.merge(vent_mapped[['Label', 'Unit', 'concept_id', 'unit_concept_id']], on='Label', how='left')\n",
    "\n",
    "# Convert string into datetime\n",
    "df_vent['Obs_time'] = pd.to_datetime(df_vent['Obs_time'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "df_vent['value'] = pd.to_numeric(df_vent['value'].astype(str), errors='coerce')\n",
    "\n",
    "# Drop rows with nan value and save into parquet\n",
    "df_vent.dropna(subset=['Obs_time', 'Label', 'value', 'concept_id'], inplace=True, ignore_index=True)\n",
    "df_vent.to_parquet(f'{input_path}/parquet/patient_ventervations.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "68a95e5a-c124-4ca6-8112-9e99409b4881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T07:57:10.805347Z",
     "iopub.status.busy": "2023-11-19T07:57:10.804951Z",
     "iopub.status.idle": "2023-11-19T07:57:10.815604Z",
     "shell.execute_reply": "2023-11-19T07:57:10.814870Z",
     "shell.execute_reply.started": "2023-11-19T07:57:10.805313Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12582900 entries, 0 to 12582899\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   PID              object        \n",
      " 1   Obs_time         datetime64[ns]\n",
      " 2   Label            object        \n",
      " 3   value            float64       \n",
      " 4   Unit             object        \n",
      " 5   concept_id       object        \n",
      " 6   unit_concept_id  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(4)\n",
      "memory usage: 672.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_vent.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "02a43ba6-4f08-420b-b802-cf161dee689d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T08:00:04.029821Z",
     "iopub.status.busy": "2023-11-19T08:00:04.029423Z",
     "iopub.status.idle": "2023-11-19T08:00:04.155822Z",
     "shell.execute_reply": "2023-11-19T08:00:04.154895Z",
     "shell.execute_reply.started": "2023-11-19T08:00:04.029787Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### I/O table ###\n",
    "df_io['IO_datetime'] = pd.to_datetime(df_io['IO_datetime'].astype(str), format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "df_io.to_parquet(f'{input_path}/parquet/patient_input_output.parquet')\n",
    "\n",
    "### Procedure Events table ###\n",
    "df_events['Event_time'] = pd.to_datetime(df_events['Event_time'].astype(str), format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "df_events.to_parquet(f'{input_path}/parquet/patient_procedure_events.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba2abc-71d4-4ee2-9244-99c9a5c8eb0d",
   "metadata": {},
   "source": [
    "# Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "34c03731-e9cd-45e8-84b9-e2172bd59b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T08:10:16.478087Z",
     "iopub.status.busy": "2023-11-19T08:10:16.477705Z",
     "iopub.status.idle": "2023-11-19T08:10:16.485088Z",
     "shell.execute_reply": "2023-11-19T08:10:16.484314Z",
     "shell.execute_reply.started": "2023-11-19T08:10:16.478052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path = 'MOVER_ETL/sis'\n",
    "\n",
    "# Make directory to save the results\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "if not os.path.exists(f'{save_path}/sample'):\n",
    "    os.mkdir(f'{save_path}/sample')    \n",
    "    \n",
    "\n",
    "# start_index for each table_id\n",
    "start_index = {\n",
    "'person': 1000000,\n",
    "'observation_period': 2000000,\n",
    "'visit_occurrence': 3000000,\n",
    "'visit_detail': 4000000,\n",
    "'condition_occurrence': 5000000,\n",
    "'drug_exposure': 6000000,\n",
    "'procedure_occurrence': 7000000,\n",
    "'measurement': 8000000,\n",
    "'death': 9000000,\n",
    "'note': 10000000,\n",
    "'location': 20000000 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f4a59c27-e70b-4152-b1f7-4163abf087a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T08:10:17.998087Z",
     "iopub.status.busy": "2023-11-19T08:10:17.997761Z",
     "iopub.status.idle": "2023-11-19T08:10:18.113548Z",
     "shell.execute_reply": "2023-11-19T08:10:18.112808Z",
     "shell.execute_reply.started": "2023-11-19T08:10:17.998056Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty dataframe with defined columns for PERSON table\n",
    "df_person = pd.DataFrame()\n",
    "\n",
    "# Assign unique IDs to each distinct 'PID' from the operations data\n",
    "unique_ids = df_info['PID'].unique()\n",
    "df_person['PERSON_ID'] = start_index['person'] + np.arange(1, len(unique_ids) + 1)\n",
    "df_person['PID'] = unique_ids\n",
    "\n",
    "# Merge relevant columns from the operations dataframe with the PERSON dataframe based on 'PID'\n",
    "usecols = ['PID', 'Age', 'Surgery_start', 'Gender']\n",
    "df_person = df_person.merge(df_info[usecols], on = 'PID', how='left')\n",
    "# Ensure only the latest discharge_time is retained for each unique PERSON_ID\n",
    "df_person.drop_duplicates(subset = 'PERSON_ID', keep = 'last', inplace = True, ignore_index = True)\n",
    "\n",
    "# Map gender values ('M' or 'F') to corresponding GENDER_CONCEPT_ID values\n",
    "df_person['GENDER_CONCEPT_ID'] = df_person['Gender'].map({'Male': 8507, 'Female': 8532})\n",
    "\n",
    "# Remove any rows with missing gender values\n",
    "df_person.dropna(subset=['GENDER_CONCEPT_ID'])\n",
    "\n",
    "# Calculate and assign the year of birth based on age and the start date\n",
    "df_person['YEAR_OF_BIRTH'] = df_person['Surgery_start'].dt.year - df_person['Age']\n",
    "# Compute the exact birth datetime using age and start date\n",
    "df_person['BIRTH_DATETIME'] = df_person['YEAR_OF_BIRTH'].apply(lambda x: datetime(year=x, month=1, day=1))\n",
    "\n",
    "# Set RACE_CONCEPT_ID to indicate all individuals are\n",
    "#df_person['RACE_CONCEPT_ID']\n",
    "\n",
    "# Assign default values for LOCATION_ID (2 for MOVER-SIS)\n",
    "df_person['LOCATION_ID'] = 2\n",
    "#df_person['PROVIDER_ID'] = 0\n",
    "\n",
    "# Populate source value columns based on values from the operations data\n",
    "df_person['PERSON_SOURCE_VALUE'] = df_person['PID']\n",
    "df_person['GENDER_SOURCE_VALUE'] = df_person['Gender']\n",
    "#df_person['RACE_SOURCE_VALUE'] = df_person['race']\n",
    "#df_person['RACE_SOURCE_CONCEPT_ID'] = 8515\n",
    "\n",
    "# Remove columns that aren't part of the final PERSON table format\n",
    "df_person.drop(columns=usecols, inplace=True)\n",
    "\n",
    "# Write the processed data to a CSV file\n",
    "df_person.to_csv(f'{save_path}/MOVER_PERSON.csv', index=False)\n",
    "df_person.to_parquet(f'{save_path}/MOVER_PERSON.parquet')\n",
    "# sample\n",
    "df_person[:1000].to_csv(f'{save_path}/sample/MOVER_PERSON.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598d9fa-e98a-4166-8944-c31ed939c263",
   "metadata": {},
   "source": [
    "# Observation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a38aa5-1e13-4e57-b2f5-da6f5dc54aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for OBSERVATION_PERIOD table\n",
    "df_obs = pd.DataFrame()\n",
    "\n",
    "# Copy PERSON_ID from PERSON table to OBSERVATION_PERIOD_ID and PERSON_ID columns in OBSERVATION_PERIOD table\n",
    "df_obs['PERSON_ID'] = df_person['PERSON_ID']\n",
    "df_obs['OBSERVATION_PERIOD_ID'] = start_index['observation_period'] + np.arange(1, len(df_obs) + 1)\n",
    "\n",
    "# Copy PERSON_SOURCE_VALUE from PERSON table to MRN in OBSERVATION_PERIOD table for merging purposes\n",
    "df_obs['MRN'] = df_person['PERSON_SOURCE_VALUE']\n",
    "\n",
    "\n",
    "usecols = ['MRN', 'HOSP_ADMSN_TIME', 'HOSP_DISCH_TIME']\n",
    "# Get the earliest admission time and latest discharge time for each MRN\n",
    "grouped = df_info.groupby('MRN').agg({\n",
    "    'HOSP_ADMSN_TIME': 'min',\n",
    "    'HOSP_DISCH_TIME': 'max'\n",
    "}).reset_index()\n",
    "# Merge to observation table\n",
    "df_obs = df_obs.merge(grouped, on='MRN', how='left')\n",
    "\n",
    "# Set the OBSERVATION_PERIOD_START_DATE to admission time\n",
    "df_obs['OBSERVATION_PERIOD_START_DATE'] = pd.to_datetime(df_obs['HOSP_ADMSN_TIME'].dt.date)\n",
    "# Set the OBSERVATION_PERIOD_END_DATE to discharge time\n",
    "df_obs['OBSERVATION_PERIOD_END_DATE'] = pd.to_datetime(df_obs['HOSP_DISCH_TIME'].dt.date)\n",
    "\n",
    "# Assign the PERIOD_TYPE_CONCEPT_ID indicating the data source is an EHR\n",
    "df_obs['PERIOD_TYPE_CONCEPT_ID'] = 32817\n",
    "\n",
    "# Remove columns that aren't part of the final OBSERVATION_PERIOD table format\n",
    "df_obs.drop(columns=usecols, inplace=True)\n",
    "\n",
    "# Write the processed data to a CSV file\n",
    "df_obs.to_csv(f'{save_path}/MOVER_OBSERVATION_PERIOD.csv', index=False)\n",
    "df_obs.to_parquet(f'{save_path}/MOVER_OBSERVATION_PERIOD.parquet')\n",
    "# sample\n",
    "df_obs[:1000].to_csv(f'{save_path}/sample/MOVER_OBSERVATION_PERIOD.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f8c37c33-a571-4a3d-9b62-210edcd0074a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T06:48:19.933585Z",
     "iopub.status.busy": "2023-11-20T06:48:19.933179Z",
     "iopub.status.idle": "2023-11-20T06:48:20.735006Z",
     "shell.execute_reply": "2023-11-20T06:48:20.733655Z",
     "shell.execute_reply.started": "2023-11-20T06:48:19.933551Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/anaconda-3-2020.02/envs/hskim/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "pip 21.3.1 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)\n"
     ]
    }
   ],
   "source": [
    "!pip -V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hskim",
   "language": "python",
   "name": "hskim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
